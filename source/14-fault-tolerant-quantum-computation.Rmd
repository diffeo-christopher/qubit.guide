# Fault-tolerant quantum computation {#fault-tolerant-quantum-computation}

> About more classical error correction and the **Hamming code**, how they generalise to quantum codes called **CSS codes**, including the **[[7,1,3]]-code**.
> Also about a family of quantum codes known as the **surface code** which have large distances, and stabilisers that can all be measured on small numbers of neighbouring qubits.
> Finally about computing in the presence of errors: logical gates and **fault tolerance**.

::: {.todo latex=""}
<!-- TO-DO -->
:::





## The Hamming code {#the-hamming-code}

The challenge in designing efficient error-correcting codes resides in the trade-off between rate and distance.
Ideally, both quantities should be high: a high rate signifies low overhead in the encoding process (i.e. requiring only a few redundant bits), and a high distance means that many errors can be corrected.
So can we optimise both of these quantities simultaneously?
Unfortunately, various established bounds tell us that there is always a trade off, so high-rate codes must have low distance, and high-distance codes must have a low rate.
Still, there is a lot of ingenuity that goes into designing good error correction codes, and some are still better than others!

Before looking at quantum codes in more depth, we again start with *classical* codes.
For example, in Section \@ref(the-classical-repetition-code) we saw the three-bit repetition code, which has a rate of $R=1/3$ and distance $3$.
However, the **$[7,4,3]$-Hamming code**^[In the late 1940s, [Richard Hamming](https://en.wikipedia.org/wiki/Richard_Hamming), working at Bell Labs, was exasperated by the fact that the machines running his punch cards (in these heroic times of computer science, punch cards were the state of the art data storage) were good enough to notice *when* there was an error (and halting) but not good enough to know *how* to fix it.] has the same distance, but a better rate of $R=4/7>1/3$.
Figure \@ref(fig:hamming-code-diagrams) show diagrammatic representations of this Hamming code, which we will now study further.

(ref:hamming-code-diagrams-caption) *Left:* The Venn diagram for the $[7,4,3]$-Hamming code. *Right:* The **finite projective plane** diagram for the same code. In both, $d_i$ are the **data bits** and the $p_i$ are the **parity bits**. The coloured circles (resp. coloured quadrilaterals) are called **plaquettes**.

```{r hamming-code-diagrams,engine='tikz',fig.width=5.5,fig.cap='(ref:hamming-code-diagrams-caption)'}
\definecolor{primary}{RGB}{177,98,78}
\definecolor{secondary}{RGB}{91,132,177}
\definecolor{tertiary}{RGB}{175,195,62}
\usetikzlibrary{backgrounds}
\begin{tikzpicture}[scale=1.2]
	\begin{scope}[scale=0.65]
		\draw[fill=primary!60,opacity=0.5] (0,0) circle (2);
		\draw[fill=secondary!60,opacity=0.5] (1,1.7) circle (2);
		\draw[fill=tertiary!60,opacity=0.5] (2,0) circle (2);
		\node[] (p1) at (1,2.5) {\footnotesize$p_1$};
		\node[] (p2) at (-0.8,-0.7) {\footnotesize$p_2$};
		\node[] (p3) at (2.8,-0.7) {\footnotesize$p_3$};
		\node[] (d1) at (-0.3,1.2) {\footnotesize$d_1$};
		\node[] (d2) at (2.3,1.2) {\footnotesize$d_2$};
		\node[] (d3) at (1,-0.7) {\footnotesize$d_3$};
		\node[] (d4) at (1,0.5) {\footnotesize$d_4$};
	\end{scope}
	\begin{scope}[xscale=0.65,yscale=0.6,shift={(8,0)}]
		\node[fill=white,draw=black,circle,inner sep=0.75mm] (p1) at (0,3) {\footnotesize$p_1$};
		\node[fill=white,draw=black,circle,inner sep=0.75mm] (p2) at (-2.6,-1.5) {\footnotesize$p_2$};
		\node[fill=white,draw=black,circle,inner sep=0.75mm] (p3) at (2.6,-1.5) {\footnotesize$p_3$};
		\node[fill=white,draw=black,circle,inner sep=0.75mm] (d1) at (-1.2975,0.75) {\footnotesize$d_1$};
		\node[fill=white,draw=black,circle,inner sep=0.75mm] (d2) at (1.2975,0.75) {\footnotesize$d_2$};
		\node[fill=white,draw=black,circle,inner sep=0.75mm] (d3) at (0,-1.5) {\footnotesize$d_3$};
		\node[fill=white,draw=black,circle,inner sep=0.75mm] (d4) at (0,0) {\footnotesize$d_4$};
	  \begin{scope}[on background layer]
	  	\draw[fill=primary!60] (p2.center) to (d1.center) to (d4.center) to (d3.center) to cycle;
	  	\draw[fill=secondary!60] (p1.center) to (d2.center) to (d4.center) to (d1.center) to cycle;
	  	\draw[fill=tertiary!60] (p3.center) to (d3.center) to (d4.center) to (d2.center) to cycle;
	  \end{scope}
	\end{scope}
\end{tikzpicture}
```

:::: {.technical title="The Fano plane" latex="{The Fano plane}"}
::: {.todo latex=""}
<!-- TO-DO: technical on finite projective plane and projective codes (somewhere in the text where it makes sense, probably near the end?) -->
:::
::::

Both diagrams in Figure \@ref(fig:hamming-code-diagrams) describe the same situation, but although the right-hand one is useful for understanding the geometry hidden in the construction and allowing us to generalise to create new codes, and is thus the one that we will tend to use, the Venn diagram on the left-hand side is maybe more suggestive of what's going on.

The idea is that we have a four-bit string $d_1d_2d_3d_4$ consisting of the four **data bits**, and we encode into a seven-bit string^[Sometimes you will see the $[7,4,3]$-Hamming code referred to simply as the **seven-bit Hamming code**.] $d_1d_2d_3d_4p_1p_2p_3$ by appending three **parity bits** $p_1$, $p_2$, and $p_3$, which are defined by
$$
	\begin{aligned}
		p_1
		&= d_1 + d_2 + d_4 \mod{2}
	\\p_2
		&= d_1 + d_3 + d_4 \mod{2}
	\\p_3
		&= d_2 + d_3 + d_4 \mod{2}.
	\end{aligned}
$$
You can hopefully see how the triangular diagram in Figure \@ref(fig:hamming-code-diagrams) tells us which data bits are used in defining each parity bit: we take the sum of all data bits in the same **plaquette** (one of the three coloured quadrilaterals) as the parity bit.

We can also express this encoding in matrix notation, defining the **data vector** $\mathbf{d}$ by
$$
	\mathbf{d}
	= \begin{bmatrix}
		d_1\\d_2\\d_3\\d_4
	\end{bmatrix}
$$
and the **generator matrix**^[Many sources define $G$ to be the transpose of what we use here, but this is just a question of convention. Because of this, we'll be dealing with the *column* (not row) spaces of matrices, also known as the **range**.] $G$ by
$$
	G
	= \begin{bmatrix}
		\begin{array}{cccc}
			1 & 0 & 0 & 0
		\\0 & 1 & 0 & 0
		\\0 & 0 & 1 & 0
		\\0 & 0 & 0 & 1
		\\\hline
			1 & 1 & 0 & 1
		\\1 & 0 & 1 & 1
		\\0 & 1 & 1 & 1
		\end{array}
	\end{bmatrix}
$$

The vector space spanned by the columns of the generator matrix $G$ is known as the **codespace** of the code, and any vector in this space is known as a **codeword**.

The encoding process is then given by the matrix $G$ acting on the vector $\mathbf{d}$.
Indeed, since the top $(4\times4)$ part of $G$ is the identity, the first four rows of the output vector $G\mathbf{d}$ will simply be a copy of $\mathbf{d}$; the bottom $(3\times4)$ part of $G$ is chosen precisely so that the last three rows of $G\mathbf{d}$ will be exactly $p_1$, $p_2$, and $p_3$.
In other words,
$$
	G\mathbf{d}
	= \begin{bmatrix}
		d_1\\d_2\\d_3\\d_4\\p_1\\p_2\\p_3
	\end{bmatrix}
	= \begin{bmatrix}
		\mathbf{d}\\p_1\\p_2\\p_3
	\end{bmatrix}.
$$

By construction, the sum of the four bits in any single plaquette of the code sum to zero.^[Since we are working with classical bits, all addition is taken $\mod{2}$, so sometimes we will neglect to say this explicitly.]
For example, in the bottom-left (red) plaquette,
$$
	\begin{aligned}
		p_2 + d_1 + d_3 + d_4
		&= d_1 + d_3 + d_4 + d_1 + d_3 + d_4
	\\&= 2(d_1 + d_3 + d_4)
	\\&= 0
	\end{aligned}
$$
and the same argument holds for the other two plaquettes.
This incredibly simple fact is where the power of the Hamming code lies, since it tells the receiver of the encoded string a lot of information about potential errors.

Let's consider a concrete example.
Say that Alice encodes her data string $d_1d_2d_3d_4$ and sends the result $G\mathbf{d}$ to Bob, who takes this vector and looks at the sum of the bits in each plaquette, and obtains the following:^[We don't write the values of the bits, only the sums of the bits in each plaquette. This is because we don't need to know the value of the bits in order to know where the error is, only the three sums!]

```{r,engine='tikz',fig.width=2.5}
\definecolor{primary}{RGB}{177,98,78}
\definecolor{secondary}{RGB}{91,132,177}
\definecolor{tertiary}{RGB}{175,195,62}
\usetikzlibrary{backgrounds}
\begin{tikzpicture}[xscale=0.65,yscale=0.6]
	\node[fill=white,draw=black,circle,inner sep=0.75mm] (p1) at (0,3) {\footnotesize$p_1$};
	\node[fill=white,draw=black,circle,inner sep=0.75mm] (p2) at (-2.6,-1.5) {\footnotesize$p_2$};
	\node[fill=white,draw=black,circle,inner sep=0.75mm] (p3) at (2.6,-1.5) {\footnotesize$p_3$};
	\node[fill=white,draw=black,circle,inner sep=0.75mm] (d1) at (-1.2975,0.75) {\footnotesize$d_1$};
	\node[fill=white,draw=black,circle,inner sep=0.75mm] (d2) at (1.2975,0.75) {\footnotesize$d_2$};
	\node[fill=white,draw=black,circle,inner sep=0.75mm] (d3) at (0,-1.5) {\footnotesize$d_3$};
	\node[fill=white,draw=black,circle,inner sep=0.75mm] (d4) at (0,0) {\footnotesize$d_4$};
  \begin{scope}[on background layer]
  	\draw[fill=primary!60] (p2.center) to (d1.center) to (d4.center) to (d3.center) to cycle;
  	\draw[fill=secondary!60] (p1.center) to (d2.center) to (d4.center) to (d1.center) to cycle;
  	\draw[fill=tertiary!60] (p3.center) to (d3.center) to (d4.center) to (d2.center) to cycle;
  \end{scope}
  \node (l) at (-1.1,-0.65) {$0$};
  \node (r) at (1.1,-0.65) {$1$};
  \node (t) at (0,1.25) {$1$};
\end{tikzpicture}
```

*If we make the assumption that at most one error occurs*, then this result tells us exactly where the bit-flip happened: it is *not* in the bottom-left (red) plaquette, but it *is* in both the top (blue) and bottom-right (yellow) plaquettes.
Looking at the diagram^[Here you might find it easier to use the Venn diagram] we see that it must be $d_2$ that was flipped, and so we can correct for this error by simply flipping it back before decoding (note that the decoding process is given by simply forgetting the last three bits of the received string).

We can describe the error location process in terms of matrices as well, using the **parity-check matrix**^[Here is yet another $H$, to go with the Hadamard and the Hamiltonian...] $H$, given by
$$
	H
	= \begin{bmatrix}
		\begin{array}{cccc|ccc}
			1 & 1 & 0 & 1 & 1 & 0 & 0
		\\1 & 0 & 1 & 1 & 0 & 1 & 0
		\\0 & 1 & 1 & 1 & 0 & 0 & 1
		\end{array}
	\end{bmatrix}.
$$
Note that the rows of $H$ are exactly the coefficients of the parity-check equations for each plaquette, where we order them top--left--right (blue--red--yellow).
For example, to get the sum corresponding to the bottom-left (red) plaquette, we need to sum the first, third, fourth, and sixth bits of the encoded string $d_1d_2d_3d_4p_1p_2p_3$, and these are exactly the non-zero entries of the second row of $H$.
The columns of the parity-check matrix $H$ are known as the **error syndromes**, for reasons we will now explain

The parity-check matrix $H$ is defined exactly so that^[Recall that codewords are exactly those vectors of the form $G\mathbf{d}$ for some data vector $\mathbf{d}$]
$$
	H\mathbf{c}
	= 0
	\iff
	\mathbf{c}\text{ is a codeword}.
$$
Now we can see a bit more into how things work, since linearity of matrix multiplication tells us that, if a receiver receives $\mathbf{c}+\mathbf{e}$ where $\mathbf{e}$ is the error,
$$
	\begin{aligned}
		H(\mathbf{c}+\mathbf{e})
		&= H\mathbf{c}+H\mathbf{e}
	\\&=H\mathbf{e}.
	\end{aligned}
$$
Decoding the message then consists of finding the most probable error $\mathbf{e}$ that yields the output $H\mathbf{e}$.
If $\mathbf{e}$ is a single bit flip error, then $H\mathbf{e}$ is exactly a column of $H$, which justifies us describing the columns as error syndromes.
We can construct a table describing all of the possible error syndromes, and which bit they indicate for us to correct:

| **Syndrome** | $000$ | $110$ | $101$ | $011$ | $111$ | $100$ | $010$ | $001$ | 
| :--- | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **Correction** | - | $d_1$ | $d_2$ | $d_3$ | $d_4$ | $p_1$ | $p_2$ | $p_3$ |

There is another common way of diagrammatically representing parity-check matrices, as a **Tanner graph**.
This is a bipartite graph^[A graph (which is a collection of **nodes** and **edges** between some of them) is **bipartite** if the nodes can be split into two sets such that all the edges go from one element of the first set to one element of the second.] consisting of two types of nodes: the codeword nodes (one for each bit of the codeword, drawn as circles), and the syndrome nodes (one for each bit of the syndrome, drawn as squares).
The edges in the Tanner graph are such that the parity-check matrix $H$ is exactly the **adjacency matrix** of the graph, i.e. the matrix that has a $1$ in the $(i,j)$-th position if the $i$-th syndrome node is connected to the $j$-th codeword node, and a $0$ otherwise.

(ref:hamming-code-tanner-graph-caption) The Tanner graph for the $[7,4,3]$-Hamming code. Note the patterns : $d_1$, $d_2$, and $d_3$ are each connected to two syndrome bits, $d_4$ is connected to three, and $p_1$, $p_2$, and $p_3$ are each connected to just one.

```{r hamming-code-tanner-graph,engine='tikz',fig.width=4,fig.cap='(ref:hamming-code-tanner-graph-caption)'}
\definecolor{primary}{RGB}{177,98,78}
\definecolor{secondary}{RGB}{91,132,177}
\begin{tikzpicture}
	\node[draw,fill=primary!60,circle,minimum size=8mm] (d1) at (-3,2) {$d_1$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (d2) at (-2,2) {$d_2$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (d3) at (-1,2) {$d_3$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (d4) at (0,2) {$d_4$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (p1) at (1,2) {$p_1$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (p2) at (2,2) {$p_2$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (p3) at (3,2) {$p_3$};
	%
	\node[draw,fill=secondary!60,minimum size=8mm] (s1) at (-2,0) {$s_1$};
	\node[draw,fill=secondary!60,minimum size=8mm] (s2) at (0,0) {$s_2$};
	\node[draw,fill=secondary!60,minimum size=8mm] (s3) at (2,0) {$s_3$};
	%
	\draw (d1.south) to (s1.north);
	\draw (d1.south) to (s2.north);
	\draw (d2.south) to (s1.north);
	\draw (d2.south) to (s3.north);
	\draw (d3.south) to (s2.north);
	\draw (d3.south) to (s3.north);
	\draw (d4.south) to (s1.north);
	\draw (d4.south) to (s2.north);
	\draw (d4.south) to (s3.north);
	\draw (p1.south) to (s1.north);
	\draw (p2.south) to (s2.north);
	\draw (p3.south) to (s3.north);
\end{tikzpicture}
```

This construction of the $[7,4,3]$-Hamming code can be generalised to result in a $[2^r-1,2^r-r-1,3]$-Hamming code^[How do we know that the distance is always 3? Well, there are *triples* of columns in the parity-check matrix that, when added together, give all zeros. This means that there are sets of 3 errors such that, if they all occur together, the syndrome will be zero, and so the distance is no more than 3. Meanwhile, all the columns are distinct, so no *pair* of columns add together trivially, which means that the distance must be greater than 2.] for any $r\geq2$, where each column of the parity-check matrix is a different binary string, excluding the string of all $0$ bits.
It's noteworthy that only a logarithmic number of parity checks are necessary to correct all single-bit errors.
However, there are some downsides to Hamming codes.
The rate $R=(2^r-r-1)/(2^r-1)$ approaches $1$ as $r\to\infty$, but Hamming codes are impractical in highly noisy environments because they have a fixed distance of $3$.

Hamming codes are special cases of a larger family of codes called **linear codes**: one in which the codewords form a vector space.
These are constructed by judicious choices of the generators matrices or parity-check matrices (since one determines the other), and can offer different trade-offs between the code rates and distances.

:::: {.technical title="Multiple-bit errors in the Hamming code" latex="{Multiple-bit errors in the Hamming code}"}
::: {.todo latex=""}
<!-- TO-DO: talk about multiple-bit errors -->
:::
::::

One last piece of the puzzle that we need to understand is the notion of dual codes.
We will write $\range(G)$ to mean the vector space spanned by the columns of the matrix $G$.
Given a code^[Usually, for linear codes, people talk of the code $C$ as being equal to the codespace, i.e. the span of the columns of $G$. For now, however, it is notationally simpler to denote a code by these two key matrices.] $C=(G,H)$ expressed in terms of its generator matrix and parity-check matrix, we know that the columns of $G$ span the kernel of $H$, i.e. that $\range(G)=\ker(H)$.
Why is this?
Well, because this is equivalent to saying that a vector is in the span of the columns of $G$ exactly when it is of the form $G\mathbf{d}$ for some data vector $\mathbf{d}$, i.e. exactly when it is a codeword, and we have already shown this above.
In particular then,
$$
	H\cdot G
	= 0
$$
(which merely says that $\range(G)\subseteq\ker(H)$).

But, taking the transpose of the above equality, we see that
$$
	G^T\cdot H^T
	= 0
$$
and so^[This tells us that $\range(H^T)\subseteq\ker(G^T)$, but we can show that this is an equality using the fact that $\range(G)=\ker(H)$ is an equality.] we can define a code $C^\perp=(H^T,G^T)$, whose codewords are exactly the columns of $H^T$, i.e. the rows of $H$.
This is known as the **dual code** (of $C$).
Since $G$ has $n$ rows and $k$ columns, and $H$ has $n-k$ rows and $n$ columns, we see that the dimension of the codespace of $C$ (i.e. the span of the columns of $G$) and the dimension of the codespace of $C^\perp$ (i.e. the span of the rows of $H$) must sum to $n$.
In fact, if $C$ is an $[n,k,d]$-code, then $C^\perp$ is an $[n,n-k,d']$ code, where $d'$ is not usually related to $d$ in any obvious way.
A nice example is the $[7,4,3]$-Hamming code, whose dual is a $[7,3,4]$-code known as the **shortened Hadamard code**.

It is immediate that $(C^\perp)^\perp=C$, but it is interesting to ask about the relationship between $C$ and $C^\perp$.
Then we say that a code is **weakly self-dual** (or **self-orthogonal**) if $\range(G)\subseteq\range(H^T)$, and **self-dual** if $\range(G)=\range(H^T)$.
In other words, a code is weakly-self dual if its codespace is a subspace of the codespace of its dual, and self-dual if these two codespaces are actually equal.
We said above that $\dim\range(G)+\dim\range(H^T)=n$, so we see that for a code to be self-dual it must be the case that $n$ is even, but this is only a necessary condition, not a sufficient one!

::: {.idea latex=""}
An $[n,k,d]$-linear code $C$ is described by two matrices:

- The **generator** matrix $G$, which is an $(n\times k)$ matrix
	$$
		G =
		\begin{bmatrix}
			\begin{array}{c}
				\id_{k\times k}
			\\\hline
				P
			\end{array}
		\end{bmatrix}
	$$
	for some $((n-k)\times(n-k))$ matrix $P$.
	The columns of $G$ are **codewords**, and and form a basis for the **codespace** $\range(G)$.
- The **parity-check** matrix $H$, which is an $((n-k)\times n)$ matrix
	$$
		H =
		\begin{bmatrix}
			\begin{array}{c|c}
				Q & \id_{(n-k)\times(n-k)}
			\end{array}
		\end{bmatrix}
	$$
	for some $((n-k)\times k)$ matrix $Q$.
	The columns of $H$ are **error syndromes**.

The matrices $G$ and $H$ have to satisfy one of the two equivalent conditions
$$
	\range(G)
	= \ker(H)
	\qquad\text{or}\qquad
	\range(H^T)
	= \ker(G^T).
$$
We can guarantee this by taking $Q=-P$.

Finally, given a code $C=(G,H)$, its **dual code** is $C^\perp=(H^T,G^T)$.
:::





## Quantum codes from classical {#quantum-codes-from-classical}

We would like to use the insights gained from our study of classical codes to help us build quantum codes.
Let's start with a classical $[n,k,d]$-code (such as the Hamming $[7,4,3]$), with parity-check matrix $H$ and generator $G$.
Each row $r$ of $H$ is a binary string $x_r=x_{r,1}x_{r,2}\ldots x_{r,n}$, where $x_{i,j}$ is the $(i,j)$-th element of $H$.
For $1\leq r\leq n$, we define a stabiliser generator
$$
	G_r
	\coloneqq X_{x_r}
	\coloneqq \otimes_{j=1}^n X^{x_{r,j}}.
$$
For example, in the case of the $[7,4,3]$-Hamming code, we have
$$
	H
	= \begin{bmatrix}
		\begin{array}{cccc|ccc}
			1 & 1 & 0 & 1 & 1 & 0 & 0
		\\1 & 0 & 1 & 1 & 0 & 1 & 0
		\\0 & 1 & 1 & 1 & 0 & 0 & 1
		\end{array}
	\end{bmatrix}.
$$
so the three rows define three generators
$$
	\begin{aligned}
		G_1
		&= X_{1101100}
		= XX\id XX\id\id
	\\G_2
		&= X_{1011010}
		= X\id XX\id X\id
	\\G_3
		&= X_{0111001}
		= \id XXX\id\id X.
	\end{aligned}
$$

Now consider a state $\ket{\psi}$ that is stabilised by these generators, i.e. such that
$$
	G_r\ket{\psi}
	= \ket{\psi}.
$$
What happens if a $Z$ error occurs on a particular qubit: what measurement results do we get when we measure the stabilisers?
Well, writing $Z_j$ to mean a $Z$ error on the $j$-th qubit, as usual,
$$
	\begin{aligned}
		G_r Z_j\ket{\psi}
		&= (-1)^{x_{r,j}} G_r\ket{\psi}
	\\&= (-1)^{x_{r,j}} \ket{\psi}.
	\end{aligned}
$$
So the measurement outcome directly corresponds to the $(r,j)$-th entry $x_{r,j}$ of the parity check matrix.
Generally, if this $Z_j$ error occurs, then measuring for all rows $r$ will give measurement outcomes that directly correspond to the $j$-th column of the parity check matrix.
This is just the same lookup table as in the classical case: this codespace is a distance $d$ error correcting code for single $Z$ errors.
Using the $[7,4,3]$-Hamming code as an example again, we get the following table:

| **Error:** | - | $Z_1$ | $Z_2$ | $Z_3$ | $Z_4$ | $Z_5$ | $Z_6$ | $Z_7$ |
| -------: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **$G_1$ outcome:** | $+1$ | $-1$ | $-1$ | $+1$ | $-1$ | $-1$ | $+1$ | $+1$ |
| **$G_2$ outcome:** | $+1$ | $-1$ | $+1$ | $-1$ | $-1$ | $+1$ | $-1$ | $+1$ |
| **$G_3$ outcome:** | $+1$ | $+1$ | $-1$ | $-1$ | $-1$ | $+1$ | $+1$ | $-1$ |

So if we measure the three stabilisers and get the measurement sequence $(-1,1,1)$ then the corresponding bit string $(1,0,0)$ in the $[7,4,3]$-Hamming code tells us that there was an error on $p_1$, i.e. a $Z_5$ error.

We have used a classical code to help us correct for $Z$ errors in the quantum case.
If we take a second classical code, with parameters $[n,k',d']$ and parity-check matrix $H'$, and use it to define $Z$-type stabilisers $G'_r=Z_{x_r}$ then we will have a distance $d'$ protection against $X$ errors.
However, we cannot simply pick the two classical codes arbitrary: if the scheme is the work, then the $X$-type and $Z$-type stabilisers must actually *be* stabilisers, i.e. they must commute.
How can we tell if this happens?
Well, if $x$ is a row of $H$, and $z$ a row of $H'$, then the we need the number of positions $i\in\{1,\ldots,n\}$ such that $x_i=z_i=1$ to be even, as these are the ones that will give operators that individually anti-commute ($XZ=-ZX$).
In notation, this is the same as asking that
$$
	x\cdot z
	\equiv 0\mod{2}.
$$
This is the same as saying that $z$, which is a row of $H'$, must be a codeword of the first code: by definition of the parity-check matrix $H$, we need that $Hz=0$.
Applying this reasoning to all rows $z$, we see that we need
$$
	H\cdot {H'}^T
	= 0
$$
or, in other words,
$$
	\range({H'}^T)
	\subseteq\ker(H).
$$
But we know that $\range(G)=\ker(H)$, and so this is equivalent to asking for
$$
	\range({H'}^T)
	\subseteq\range(G).
$$


We can figure out some key properties of combining an $[n,k,d]$-code $(G,H)$ for $X$-stabilisers and an $[n,k',d']$-code $(G',H')$ for $Z$-stabilisers without too much difficult.
Since our first code encodes $k$ bits, the generator $G$ has $k$ rows, and the parity-check matrix $H$ has $n-k$ rows.
Thus there are $n-k$ of the $X$-type generators, and $n-k'$ of the $Z$-type generators; in total there are $2n-(k+k')$ generators.
Since each generator halves the dimension, the dimension of the Hilbert space defined by the stabilisers is
$$
	2^{n-2n+k+k'}
	= 2^{k+k'-n}
$$
i.e. it encodes $k+k'-n$ qubits.
The combined code has a distance $k$ against $Z$ errors, and $k'$ against $X$ errors; since the two types of errors are correctly independently, the total distance is simply $\min(d,d')$.
In summary then, we have created an
$$
	[[n, k+k'-n, \min(d,d')]]
$$
quantum error correction code, and its decoding is well understood based on the classical decoding methods applied independently for $X$ errors and $Z$ errors.
This general construction of quantum error correcting codes is known as the **CSS construction**, for its originators [Robert Calderbank](https://en.wikipedia.org/wiki/Robert_Calderbank), [Peter Shor](https://en.wikipedia.org/wiki/Peter_Shor), and [Andrew Steane](https://en.wikipedia.org/wiki/Andrew_Steane).

::: {.idea latex=""}
Given an $[n,k,d]$-code $C_1=(G_1,H_1)$ and an $[n,k_2,d_2]$-code $C_2=(G_2,H_2)$ such that $\range(H_2^T)\subseteq\range(G_1)$, the **CSS code** $\operatorname{CSS}(C_1,C_2)$ constructed as above is an $[[n,k_1+k_2-n,\min(d_1,d_2)]]$-code.
:::

As always, one needs to be careful of conventions!
Many sources define a code to be the codespace $\range{G}$ itself instead of the pair $(G,H)$, and usually also replace $C_2$ with ${C_2}^\perp$ in the statement of the CSS construction.^[In particular, what we have defined would often be called the CSS construction of $C_1$ over ${C_2}^\perp$ (instead of over $C_2$).]

Before moving on, let's look at the remaining details of applying the CSS construction to the $[7,4,3]$-Hamming code.
Let $C_1=C_2=(G,H)$ be the $[7,4,3]$-Hamming code with $G$ and $H$ as in Section \@ref(the-hamming-code).
To apply the CSS construction, we need to check that $\range(H_2^T)\subseteq\range(G_1)$.
Since $C_1=C_2$, this is simply asking that the $[7,4,3]$-Hamming code be weakly self-dual, i.e. that
$$
	\range(H^T)
	\subseteq\range(G)
$$
which can be checked to be true by hand.
This means that we can use the $[7,4,3]$-Hamming code to define both our $X$-type and $Z$-type generators: using the notation of Section \@ref(pauli-stabilisers), the group of generators is generated by
$$
  \begin{array}{c|ccccccc|}
    + & X & X & \id & X & X & \id & \id
  \\+ & X & \id & X & X & \id & X & \id
  \\+ & \id & X & X & X & \id & \id & X
  \\+ & Z & Z & \id & Z & Z & \id & \id
  \\+ & Z & \id & Z & Z & \id & Z & \id
  \\+ & \id & Z & Z & Z & \id & \id & Z
  \end{array}
$$
The result is a $[[7,1,3]]$-code, generally attributed to Steane and thus known as the **Steane code**, that encodes one logical qubit across seven physical ones, and that is able to correct for any single-qubit Pauli error.
We can visualise the Steane code using its Tanner graph, as in Figure \@ref(fig:tanner-graph-7-1-3).

(ref:tanner-graph-7-1-3-caption) The Tanner graph of the Steane code. For CSS codes, there are two types of "parity checks": one for detecting $X$ errors (solid lines), and one for $Z$ errors (dashed lines).

```{r tanner-graph-7-1-3,engine='tikz',fig.width=4,fig.cap='(ref:tanner-graph-7-1-3-caption)'}
\definecolor{primary}{RGB}{177,98,78}
\definecolor{secondary}{RGB}{91,132,177}
\begin{tikzpicture}
	\node[draw,fill=secondary!60,minimum size=8mm] (s1) at (-2,4) {$s_1$};
	\node[draw,fill=secondary!60,minimum size=8mm] (s2) at (0,4) {$s_2$};
	\node[draw,fill=secondary!60,minimum size=8mm] (s3) at (2,4) {$s_3$};
	%
	\node[draw,fill=primary!60,circle,minimum size=8mm] (q1) at (-3,2) {$q_1$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (q2) at (-2,2) {$q_2$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (q3) at (-1,2) {$q_3$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (q4) at (0,2) {$q_4$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (q5) at (1,2) {$q_5$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (q6) at (2,2) {$q_6$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (q7) at (3,2) {$q_7$};
	%
	\node[draw,fill=secondary!60,minimum size=8mm] (s4) at (-2,0) {$s_4$};
	\node[draw,fill=secondary!60,minimum size=8mm] (s5) at (0,0) {$s_5$};
	\node[draw,fill=secondary!60,minimum size=8mm] (s6) at (2,0) {$s_6$};
	%
	\draw[dashed] (s1.south) to (q1.north);
	\draw[dashed] (s1.south) to (q2.north);
	\draw[dashed] (s1.south) to (q4.north);
	\draw[dashed] (s1.south) to (q5.north);
	\draw[dashed] (s2.south) to (q1.north);
	\draw[dashed] (s2.south) to (q3.north);
	\draw[dashed] (s2.south) to (q4.north);
	\draw[dashed] (s2.south) to (q6.north);
	\draw[dashed] (s3.south) to (q2.north);
	\draw[dashed] (s3.south) to (q3.north);
	\draw[dashed] (s3.south) to (q4.north);
	\draw[dashed] (s3.south) to (q7.north);
	\draw (q1.south) to (s4.north);
	\draw (q1.south) to (s5.north);
	\draw (q2.south) to (s5.north);
	\draw (q2.south) to (s6.north);
	\draw (q3.south) to (s4.north);
	\draw (q3.south) to (s6.north);
	\draw (q4.south) to (s4.north);
	\draw (q4.south) to (s5.north);
	\draw (q4.south) to (s6.north);
	\draw (q5.south) to (s4.north);
	\draw (q6.south) to (s5.north);
	\draw (q7.south) to (s6.north);
\end{tikzpicture}
```

A round of error correction can be performed by measuring each of the stabilisers in the standard way:

```{r,engine='tikz',engine.opts=list(template="latex/tikz2pdf.tex"),fig.width=6}
\begin{quantikz}
	\lstick[wires=6]{$\ket{0}^{\otimes6}$}
	& \gate{H}
	& \ctrl{10}
	& \qw
	& \qw
	& \qw
	& \qw
	& \qw
	& \gate{H}
	& \meter{}
\\[-1.5em]
	& \gate{H}
	& \qw
	& \ctrl{10}
	& \qw
	& \qw
	& \qw
	& \qw
	& \gate{H}
	& \meter{}
\\[-1.5em]
	& \gate{H}
	& \qw
	& \qw
	& \ctrl{10}
	& \qw
	& \qw
	& \qw
	& \gate{H}
	& \meter{}
\\[-1.5em]
	& \gate{H}
	& \qw
	& \qw
	& \qw
	& \ctrl{7}
	& \qw
	& \qw
	& \gate{H}
	& \meter{}
\\[-1.5em]
	& \gate{H}
	& \qw
	& \qw
	& \qw
	& \qw
	& \ctrl{7}
	& \qw
	& \gate{H}
	& \meter{}
\\[-1.5em]
	& \gate{H}
	& \qw
	& \qw
	& \qw
	& \qw
	& \qw
	& \ctrl{7}
	& \gate{H}
	& \meter{}
\\[-1em]
	& \qw
	& \gate{X}
	& \gate{X}
	& \qw
	& \gate{Z}
	& \gate{Z}
	& \qw
	& \qw
	& \qw
\\[-1em]
	& \qw
	& \gate{X}
	& \qw
	& \gate{X}
	& \gate{Z}
	& \qw
	& \gate{Z}
	& \qw
	& \qw
\\[-1em]
	& \qw
	& \qw
	& \gate{X}
	& \gate{X}
	& \qw
	& \gate{Z}
	& \gate{Z}
	& \qw
	& \qw
\\[-1em]
	& \qw
	& \gate{X}
	& \gate{X}
	& \gate{X}
	& \gate{Z}
	& \gate{Z}
	& \gate{Z}
	& \qw
	& \qw
\\[-1em]
	& \qw
	& \gate{X}
	& \qw
	& \qw
	& \gate{Z}
	& \qw
	& \qw
	& \qw
	& \qw
\\[-1em]
	& \qw
	& \qw
	& \gate{X}
	& \qw
	& \qw
	& \gate{Z}
	& \qw
	& \qw
	& \qw
\\[-1em]
	& \qw
	& \qw
	& \qw
	& \gate{X}
	& \qw
	& \qw
	& \gate{Z}
	& \qw
	& \qw
\end{quantikz}
```





## Logical operators {#logical-operators}

You will perhaps have noticed that we haven't yet written out what the stabiliser states actually *are*, nor what the encoding circuits look like.
There is a simple reason for this: at this point, we don't actually know!
There's a little more work to be done --- the stabilisers have provided us with a two-dimensional space, but if we have $\ket{0}$ and $\ket{1}$ to encode, how are they mapped within the space?
So far, it's undefined, and there is a lot of freedom to choose, but the structures provided by group theory are quite helpful here in providing some natural choices.

We're going to turn back all the way to Sections \@ref(normal-subgroups) and \@ref(pauli-normalisers), where we discovered how to arrange the elements of the Pauli group $\mathcal{P}_n$ into cosets once we had chosen a stabiliser $\mathcal{S}$, by using the two quotient groups $N(\mathcal{S})/\mathcal{S}$ and $\mathcal{P}_n/N(\mathcal{S})$.
How does this help us with our stabiliser error correction codes?
Let's start with the former: cosets of $\mathcal{S}$ inside its normaliser $N(\mathcal{S})$.

If $\ket{\psi}\in V_\mathcal{S}$ is a state in the stabilised subspace, then any element $g\in\mathcal{S}$ always satisfies
$$
	g\ket{\psi}
	= \ket{\psi}
$$
whereas any element $g\in N(\mathcal{S})\setminus\mathcal{S}$ merely satisfies
$$
	g\ket{\psi}
	\in V_\mathcal{S}
$$
and, for any such $g$, there are always states in $V_\mathcal{S}$ that are not mapped to themselves.

::: {.todo latex=""}
<!-- TO-DO -->
:::





<div class="video" title="Pauli operators in error correction" data-videoid="kd3ahY55DYA"></div>

<div class="video" title="Stabilizer codes" data-videoid="jI8S-2oB7fg"></div>

<div class="video" title="Seven-qubit code and transversal constructions" data-videoid="vZ5p_fJbTMc"></div>

<div class="video" title="Fault-tolerant computation and threshold theorems" data-videoid="TPUElc_4amo"></div>
