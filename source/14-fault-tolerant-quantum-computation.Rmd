# Fault-tolerant quantum computation {#fault-tolerant-quantum-computation}

> About more classical error correction and the **Hamming code**, how they generalise to quantum codes called **CSS codes**, including the **[[7,1,3]]-code**.
> Also about a family of quantum codes known as the **surface code** which have large distances, and stabilisers that can all be measured on small numbers of neighbouring qubits.
> Finally about computing in the presence of errors: logical gates and **fault tolerance**.

::: {.todo latex=""}
<!-- TO-DO -->
:::





## The Hamming code {#the-hamming-code}

The challenge in designing efficient error-correcting codes resides in the trade-off between rate and distance.
Ideally, both quantities should be high: a high rate signifies low overhead in the encoding process (i.e. requiring only a few redundant bits), and a high distance means that many errors can be corrected.
So can we optimise both of these quantities simultaneously?
Unfortunately, various established bounds tell us that there is always a trade off, so high-rate codes must have low distance, and high-distance codes must have a low rate.
Still, there is a lot of ingenuity that goes into designing good error correction codes, and some are still better than others!

Before looking at quantum codes in more depth, we again start with *classical* codes.
For example, in Section \@ref(the-classical-repetition-code) we saw the three-bit repetition code, which has a rate of $R=1/3$ and distance $3$.
However, the **$[7,4,3]$-Hamming code**^[In the late 1940s, [Richard Hamming](https://en.wikipedia.org/wiki/Richard_Hamming), working at Bell Labs, was exasperated by the fact that the machines running his punch cards (in these heroic times of computer science, punch cards were the state of the art data storage) were good enough to notice *when* there was an error (and halting) but not good enough to know *how* to fix it.] has the same distance, but a better rate of $R=4/7>1/3$.
Figure \@ref(fig:hamming-code-diagrams) show diagrammatic representations of this Hamming code, which we will now study further.

(ref:hamming-code-diagrams-caption) *Left:* The Venn diagram for the $[7,4,3]$-Hamming code. *Right:* The **finite projective plane** diagram for the same code. In both, $d_i$ are the **data bits** and the $p_i$ are the **parity bits**. The coloured circles (resp. coloured quadrilaterals) are called **plaquettes**.

```{r hamming-code-diagrams,engine='tikz',fig.width=5.5,fig.cap='(ref:hamming-code-diagrams-caption)'}
\definecolor{primary}{RGB}{177,98,78}
\definecolor{secondary}{RGB}{91,132,177}
\definecolor{tertiary}{RGB}{175,195,62}
\usetikzlibrary{backgrounds}
\begin{tikzpicture}[scale=1.2]
	\begin{scope}[scale=0.65]
		\draw[fill=primary!60,opacity=0.5] (0,0) circle (2);
		\draw[fill=secondary!60,opacity=0.5] (1,1.7) circle (2);
		\draw[fill=tertiary!60,opacity=0.5] (2,0) circle (2);
		\node[] (p1) at (1,2.5) {\footnotesize$p_1$};
		\node[] (p2) at (-0.8,-0.7) {\footnotesize$p_2$};
		\node[] (p3) at (2.8,-0.7) {\footnotesize$p_3$};
		\node[] (d1) at (-0.3,1.2) {\footnotesize$d_1$};
		\node[] (d2) at (2.3,1.2) {\footnotesize$d_2$};
		\node[] (d3) at (1,-0.7) {\footnotesize$d_3$};
		\node[] (d4) at (1,0.5) {\footnotesize$d_4$};
	\end{scope}
	\begin{scope}[xscale=0.65,yscale=0.6,shift={(8,0)}]
		\node[fill=white,draw=black,circle,inner sep=0.75mm] (p1) at (0,3) {\footnotesize$p_1$};
		\node[fill=white,draw=black,circle,inner sep=0.75mm] (p2) at (-2.6,-1.5) {\footnotesize$p_2$};
		\node[fill=white,draw=black,circle,inner sep=0.75mm] (p3) at (2.6,-1.5) {\footnotesize$p_3$};
		\node[fill=white,draw=black,circle,inner sep=0.75mm] (d1) at (-1.2975,0.75) {\footnotesize$d_1$};
		\node[fill=white,draw=black,circle,inner sep=0.75mm] (d2) at (1.2975,0.75) {\footnotesize$d_2$};
		\node[fill=white,draw=black,circle,inner sep=0.75mm] (d3) at (0,-1.5) {\footnotesize$d_3$};
		\node[fill=white,draw=black,circle,inner sep=0.75mm] (d4) at (0,0) {\footnotesize$d_4$};
	  \begin{scope}[on background layer]
	  	\draw[fill=primary!60] (p2.center) to (d1.center) to (d4.center) to (d3.center) to cycle;
	  	\draw[fill=secondary!60] (p1.center) to (d2.center) to (d4.center) to (d1.center) to cycle;
	  	\draw[fill=tertiary!60] (p3.center) to (d3.center) to (d4.center) to (d2.center) to cycle;
	  \end{scope}
	\end{scope}
\end{tikzpicture}
```

Both diagrams in Figure \@ref(fig:hamming-code-diagrams) describe the same situation, but although the right-hand one is useful for understanding the geometry hidden in the construction and allowing us to generalise to create new codes, and is thus the one that we will tend to use, the Venn diagram on the left-hand side is maybe more suggestive of what's going on.

The idea is that we have a four-bit string $d_1d_2d_3d_4$ consisting of the four **data bits**, and we encode into a seven-bit string^[Sometimes you will see the $[7,4,3]$-Hamming code referred to simply as the **seven-bit Hamming code**.] $d_1d_2d_3d_4p_1p_2p_3$ by appending three **parity bits** $p_1$, $p_2$, and $p_3$, which are defined by
$$
	\begin{aligned}
		p_1
		&= d_1 + d_2 + d_4 \mod{2}
	\\p_2
		&= d_1 + d_3 + d_4 \mod{2}
	\\p_3
		&= d_2 + d_3 + d_4 \mod{2}.
	\end{aligned}
$$
You can hopefully see how the triangular diagram in Figure \@ref(fig:hamming-code-diagrams) tells us which data bits are used in defining each parity bit: we take the sum of all data bits in the same **plaquette** (one of the three coloured quadrilaterals) as the parity bit.

We can also express this encoding in matrix notation, defining the **data vector** $\mathbf{d}$ by
$$
	\mathbf{d}
	= \begin{bmatrix}
		d_1\\d_2\\d_3\\d_4
	\end{bmatrix}
$$
and the **generator matrix**^[Many sources define $G$ to be the transpose of what we use here, but this is just a question of convention.] $G$ by
$$
	G
	= \begin{bmatrix}
		\begin{array}{cccc}
			1 & 0 & 0 & 0
		\\0 & 1 & 0 & 0
		\\0 & 0 & 1 & 0
		\\0 & 0 & 0 & 1
		\\\hline
			1 & 1 & 0 & 1
		\\1 & 0 & 1 & 1
		\\0 & 1 & 1 & 1
		\end{array}
	\end{bmatrix}
$$
The encoding process is then given by the matrix $G$ acting on the vector $\mathbf{d}$.
Indeed, since the top $(4\times4)$ part of $G$ is the identity, the first four rows of the output vector $G\mathbf{d}$ will simply be a copy of $\mathbf{d}$; the bottom $(3\times4)$ part of $G$ is chosen precisely so that the last three rows of $G\mathbf{d}$ will be exactly $p_1$, $p_2$, and $p_3$.
In other words,
$$
	G\mathbf{d}
	= \begin{bmatrix}
		d_1\\d_2\\d_3\\d_4\\p_1\\p_2\\p_3
	\end{bmatrix}
	= \begin{bmatrix}
		\mathbf{d}\\p_1\\p_2\\p_3
	\end{bmatrix}.
$$

By construction, the sum of the four bits in any single plaquette of the code sum to zero.^[Since we are working with classical bits, all addition is taken $\mod{2}$, so sometimes we will neglect to say this explicitly.]
For example, in the bottom-left (red) plaquette,
$$
	\begin{aligned}
		p_2 + d_1 + d_3 + d_4
		&= d_1 + d_3 + d_4 + d_1 + d_3 + d_4
	\\&= 2(d_1 + d_3 + d_4)
	\\&= 0
	\end{aligned}
$$
and the same argument holds for the other two plaquettes.
This incredibly simple fact is where the power of the Hamming code lies, since it tells the receiver of the encoded string a lot of information about potential errors.

Let's consider a concrete example.
Say that Alice encodes her data string $d_1d_2d_3d_4$ and sends the result $G\mathbf{d}$ to Bob, who takes this vector and looks at the sum of the bits in each plaquette, and obtains the following:^[We don't write the values of the bits, only the sums of the bits in each plaquette. This is because we don't need to know the value of the bits in order to know where the error is, only the three sums!]

```{r,engine='tikz',fig.width=2.5}
\definecolor{primary}{RGB}{177,98,78}
\definecolor{secondary}{RGB}{91,132,177}
\definecolor{tertiary}{RGB}{175,195,62}
\usetikzlibrary{backgrounds}
\begin{tikzpicture}[xscale=0.65,yscale=0.6]
	\node[fill=white,draw=black,circle,inner sep=0.75mm] (p1) at (0,3) {\footnotesize$p_1$};
	\node[fill=white,draw=black,circle,inner sep=0.75mm] (p2) at (-2.6,-1.5) {\footnotesize$p_2$};
	\node[fill=white,draw=black,circle,inner sep=0.75mm] (p3) at (2.6,-1.5) {\footnotesize$p_3$};
	\node[fill=white,draw=black,circle,inner sep=0.75mm] (d1) at (-1.2975,0.75) {\footnotesize$d_1$};
	\node[fill=white,draw=black,circle,inner sep=0.75mm] (d2) at (1.2975,0.75) {\footnotesize$d_2$};
	\node[fill=white,draw=black,circle,inner sep=0.75mm] (d3) at (0,-1.5) {\footnotesize$d_3$};
	\node[fill=white,draw=black,circle,inner sep=0.75mm] (d4) at (0,0) {\footnotesize$d_4$};
  \begin{scope}[on background layer]
  	\draw[fill=primary!60] (p2.center) to (d1.center) to (d4.center) to (d3.center) to cycle;
  	\draw[fill=secondary!60] (p1.center) to (d2.center) to (d4.center) to (d1.center) to cycle;
  	\draw[fill=tertiary!60] (p3.center) to (d3.center) to (d4.center) to (d2.center) to cycle;
  \end{scope}
  \node (l) at (-1.1,-0.65) {$0$};
  \node (r) at (1.1,-0.65) {$1$};
  \node (t) at (0,1.25) {$1$};
\end{tikzpicture}
```

*If we make the assumption that at most one error occurs*, then this result tells us exactly where the bit-flip happened: it is *not* in the bottom-left (red) plaquette, but it *is* in both the top (blue) and bottom-right (yellow) plaquettes.
Looking at the diagram^[Here you might find it easier to use the Venn diagram] we see that it must be $d_2$ that was flipped, and so we can correct for this error by simply flipping it back before decoding (note that the decoding process is given by simply forgetting the last three bits of the received string).

We can describe the error location process in terms of matrices as well, using the **parity-check matrix**^[Here is yet another $H$, to go with the Hadamard and the Hamiltonian...] $H$, given by
$$
	H
	= \begin{bmatrix}
		\begin{array}{cccc|ccc}
			1 & 1 & 0 & 1 & 1 & 0 & 0
		\\1 & 0 & 1 & 1 & 0 & 1 & 0
		\\0 & 1 & 1 & 1 & 0 & 0 & 1
		\end{array}
	\end{bmatrix}.
$$
Note that the rows of $H$ are exactly the coefficients of the parity-check equations for each plaquette, where we order them top--left--right (blue--red--yellow).
For example, to get the sum corresponding to the bottom-left (red) plaquette, we need to sum the first, third, fourth, and sixth bits of the encoded string $d_1d_2d_3d_4p_1p_2p_3$, and these are exactly the non-zero entries of the second row of $H$.
The columns of $H$ are, as we will explain in a moment, the error syndromes.

The parity-check matrix $H$ is defined exactly so that^[Mathematically, this is saying that $H$ is defined as the kernel of $G^T$ (modulo 2), and vice versa.]
$$
	H\mathbf{c}
	= 0
	\iff
	\mathbf{c}\text{ is a codeword}
$$
where codewords are exactly those vectors of the form $G\mathbf{d}$ for some data vector $d$.
Now we can see a bit more into how things work, since linearity of matrix multiplication tells us that, if a receiver receives $\mathbf{c}+\mathbf{e}$ where $\mathbf{e}$ is the error,
$$
	\begin{aligned}
		H(\mathbf{c}+\mathbf{e})
		&= H\mathbf{c}+H\mathbf{e}
	\\&=H\mathbf{e}.
	\end{aligned}
$$
Decoding the message then consists of finding the most probable error $\mathbf{e}$ that yields the output $H\mathbf{e}$.
If $\mathbf{e}$ is a single bit flip error, then $H\mathbf{e}$ is exactly a column of $H$, which justifies us describing the columns as error syndromes.
We can construct a table describing all of the possible error syndromes, and which bit they indicate for us to correct:

| **Syndrome** | $000$ | $110$ | $101$ | $011$ | $111$ | $100$ | $010$ | $001$ | 
| :--- | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **Correction** | - | $d_1$ | $d_2$ | $d_3$ | $d_4$ | $p_1$ | $p_2$ | $p_3$ |

There is another common way of diagrammatically representing parity-check matrices, as a **Tanner graph**.
This is a bipartite graph^[A graph (which is a collection of **nodes** and **edges** between some of them) is **bipartite** if the nodes can be split into two sets such that all the edges go from one element of the first set to one element of the second.] consisting of two types of nodes: the codeword nodes (one for each bit of the codeword, drawn as circles), and the syndrome nodes (one for each bit of the syndrome, drawn as squares).
The edges in the Tanner graph are such that the parity-check matrix $H$ is exactly the **adjacency matrix** of the graph, i.e. the matrix that has a $1$ in the $(i,j)$-th position if the $i$-th syndrome node is connected to the $j$-th codeword node, and a $0$ otherwise.

(ref:hamming-code-tanner-graph-caption) The Tanner graph for the $[7,4,3]$-Hamming code. Note the patterns : $d_1$, $d_2$, and $d_3$ are each connected to two syndrome bits, $d_4$ is connected to three, and $p_1$, $p_2$, and $p_3$ are each connected to just one.

```{r hamming-code-tanner-graph,engine='tikz',fig.width=4,fig.cap='(ref:hamming-code-tanner-graph-caption)'}
\definecolor{primary}{RGB}{177,98,78}
\definecolor{secondary}{RGB}{91,132,177}
\begin{tikzpicture}
	\node[draw,fill=primary!60,circle,minimum size=8mm] (d1) at (-3,2) {$d_1$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (d2) at (-2,2) {$d_2$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (d3) at (-1,2) {$d_3$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (d4) at (0,2) {$d_4$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (p1) at (1,2) {$p_1$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (p2) at (2,2) {$p_2$};
	\node[draw,fill=primary!60,circle,minimum size=8mm] (p3) at (3,2) {$p_3$};
	%
	\node[draw,fill=secondary!60,minimum size=8mm] (s1) at (-2,0) {$s_1$};
	\node[draw,fill=secondary!60,minimum size=8mm] (s2) at (0,0) {$s_2$};
	\node[draw,fill=secondary!60,minimum size=8mm] (s3) at (2,0) {$s_3$};
	%
	\draw (d1.south) to (s1.north);
	\draw (d1.south) to (s2.north);
	\draw (d2.south) to (s1.north);
	\draw (d2.south) to (s3.north);
	\draw (d3.south) to (s2.north);
	\draw (d3.south) to (s3.north);
	\draw (d4.south) to (s1.north);
	\draw (d4.south) to (s2.north);
	\draw (d4.south) to (s3.north);
	\draw (p1.south) to (s1.north);
	\draw (p2.south) to (s2.north);
	\draw (p3.south) to (s3.north);
\end{tikzpicture}
```

This construction of the $[7,4,3]$-Hamming code can be generalised to result in a $[2^r-1,2^r-r-1,3]$-Hamming code^[How do we know that the distance is always 3? Well, there are *triples* of columns in the parity-check matrix that, when added together, give all zeros. This means that there are sets of 3 errors such that, if they all occur together, the syndrome will be zero, and so the distance is no more than 3. Meanwhile, all the columns are distinct, so no *pair* of columns add together trivially, which means that the distance must be greater than 2.] for any $r\geq2$, where each column of the parity-check matrix is a different binary string, excluding the string of all $0$ bits.
It's noteworthy that only a logarithmic number of parity checks are necessary to correct all single-bit errors.
However, there are some downsides to Hamming codes.
The rate $R=(2^r-r-1)/(2^r-1)$ approaches $1$ as $r\to\infty$, but Hamming codes are impractical in highly noisy environments because they have a fixed distance of $3$.

Hamming codes are special cases of a larger family of codes called **linear codes**, which are constructed by judicious choices of the generators matrices or parity-check matrices (since one determines the other), and can offer different trade-offs between the code rates and distances.

:::: {.technical title="Multiple-bit errors in the Hamming code" latex="{Multiple-bit errors in the Hamming code}"}
::: {.todo latex=""}
<!-- TO-DO: talk about multiple-bit errors -->
:::
::::

:::: {.technical title="Projective codes" latex="{Projective codes}"}
::: {.todo latex=""}
<!-- TO-DO: technical on finite projective plane and projective codes (somewhere in the text where it makes sense, probably near the end?) -->
:::
::::





## Quantum codes from classical {#quantum-codes-from-classical}

::: {.todo latex=""}
<!-- TO-DO -->
:::






<div class="video" title="Pauli operators in error correction" data-videoid="kd3ahY55DYA"></div>

<div class="video" title="Stabilizer codes" data-videoid="jI8S-2oB7fg"></div>

<div class="video" title="Seven-qubit code and transversal constructions" data-videoid="vZ5p_fJbTMc"></div>

<div class="video" title="Fault-tolerant computation and threshold theorems" data-videoid="TPUElc_4amo"></div>
